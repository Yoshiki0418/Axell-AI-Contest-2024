{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習・推論用サンプル(notebook/Google Colaboratory版)\n",
    "\n",
    "このサンプルはnotebookを利用した学習サンプルです。  \n",
    "notebookを利用せず学習を行う方はtrain.pyを参照してください。  \n",
    "\n",
    "事前にREADME.mdを参考にGoogle Dirveに以下の形式でデータセットを配置してください・  \n",
    "\n",
    "```\n",
    "dataset/  # 配布しているデータセット\n",
    "+ train/\n",
    "+ validation/\n",
    "  + 0.25x/\n",
    "  + original/\n",
    "```\n",
    "\n",
    "また、学習済みモデルはGoogle Drive直下にonnx形式で保存されます。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step 1 依存ライブラリの導入とGoogle Driveのマウント\n",
    "学習、推論に必要なライブラリを導入し、データセットが保存されているGoogle Driveをマウントします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 環境構築(依存ライブラリを導入)\n",
    "!pip install Pillow tqdm onnx tensorboard opencv-python\n",
    "## torchのインストール(利用する環境に応じて内容は変更)\n",
    "!pip install torch torchvision torchaudio\n",
    "## onnxruntimeのインストール(推論用/利用する環境に応じて内容は変更)\n",
    "!pip install onnxruntime-gpu \n",
    "# notebook向け追加ライブラリ\n",
    "!pip install ipywidgets\n",
    "\n",
    "# Google Driveをマウント\n",
    "# セル実行時にアクセス許可の画面が表示されますのでログインして許可をしてください。\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "%cd /content/drive/MyDrive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step2 モデル構造の定義\n",
    "\n",
    "入力画像を4倍に拡大するモデルを定義します。  \n",
    "ここではサンプルとしてESPCNを用いています。  \n",
    "モデルへの入力はN, C, H, Wの4次元入力で、チャンネルはR, G, Bの順、画素値は0~1に正規化されている想定となります。  \n",
    "また、出力も同様のフォーマットで、縦横の解像度(H, W)が4倍となります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4倍拡大サンプルモデル(ESPCN)の構造定義\n",
    "# 参考 https://github.com/Nhat-Thanh/ESPCN-Pytorch\n",
    "from torch import nn, clip, tensor\n",
    "\n",
    "class ESPCN4x(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.scale = 4\n",
    "        self.conv_1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=5, padding=2)\n",
    "        nn.init.normal_(self.conv_1.weight, mean=0, std=0.001)\n",
    "        nn.init.zeros_(self.conv_1.bias)\n",
    "\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "        self.conv_2 = nn.Conv2d(in_channels=64, out_channels=32, kernel_size=3, padding=1)\n",
    "        nn.init.normal_(self.conv_2.weight, mean=0, std=0.001)\n",
    "        nn.init.zeros_(self.conv_2.bias)\n",
    "        \n",
    "        self.conv_3 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1)\n",
    "        nn.init.normal_(self.conv_3.weight, mean=0, std=0.001)\n",
    "        nn.init.zeros_(self.conv_3.bias)\n",
    "\n",
    "        self.conv_4 = nn.Conv2d(in_channels=32, out_channels=(1 * self.scale * self.scale), kernel_size=3, padding=1)\n",
    "        nn.init.normal_(self.conv_4.weight, mean=0, std=0.001)\n",
    "        nn.init.zeros_(self.conv_4.bias)\n",
    "\n",
    "        self.pixel_shuffle = nn.PixelShuffle(self.scale)\n",
    "\n",
    "    def forward(self, X_in: tensor) -> tensor:\n",
    "        X = X_in.reshape(-1, 1, X_in.shape[-2], X_in.shape[-1])\n",
    "        X = self.act(self.conv_1(X))\n",
    "        X = self.act(self.conv_2(X))\n",
    "        X = self.act(self.conv_3(X))\n",
    "        X = self.conv_4(X)\n",
    "        X = self.pixel_shuffle(X)\n",
    "        X = X.reshape(-1, 3, X.shape[-2], X.shape[-1])\n",
    "        X_out = clip(X, 0.0, 1.0)\n",
    "        return X_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step3 データセットの定義\n",
    "\n",
    "提供されている学習用画像と評価用画像セット(高解像度＋低解像度)を読み出すクラスです。  \n",
    "学習用画像は元画像を512px四方に切り出し正解画像とします。また、正解画像を1/4に縮小したものを入力画像として用います(TrainDataSet)。  \n",
    "評価用画像は高解像度と低解像度がセットで提供されているため、低解像度のものを入力画像、高解像度のものを正解画像として用います(ValidationDataSet)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセット定義\n",
    "from typing import Tuple\n",
    "import PIL\n",
    "from PIL.Image import Image\n",
    "from torch import Tensor\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "from pathlib import Path\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class DataSetBase(data.Dataset, ABC):\n",
    "    def __init__(self, image_path: Path):\n",
    "        self.images = list(image_path.iterdir())\n",
    "        self.max_num_sample = len(self.images)\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return self.max_num_sample\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_low_resolution_image(self, image: Image, path: Path)-> Image:\n",
    "        pass\n",
    "    \n",
    "    def preprocess_high_resolution_image(self, image: Image) -> Image:\n",
    "        return image\n",
    "    \n",
    "    def __getitem__(self, index) -> Tuple[Tensor, Tensor]:\n",
    "        image_path = self.images[index % len(self.images)]\n",
    "        high_resolution_image = self.preprocess_high_resolution_image(PIL.Image.open(image_path))\n",
    "        low_resolution_image = self.get_low_resolution_image(high_resolution_image, image_path)\n",
    "        return transforms.ToTensor()(low_resolution_image), transforms.ToTensor()(high_resolution_image)\n",
    "\n",
    "class TrainDataSet(DataSetBase):\n",
    "    def __init__(self, image_path: Path, num_image_per_epoch: int = 2000):\n",
    "        super().__init__(image_path)\n",
    "        self.max_num_sample = num_image_per_epoch\n",
    "\n",
    "    def get_low_resolution_image(self, image: Image, path: Path)-> Image:\n",
    "        return transforms.Resize((image.size[0] // 4, image.size[1] // 4), transforms.InterpolationMode.BICUBIC)(image.copy())\n",
    "    \n",
    "    def preprocess_high_resolution_image(self, image: Image) -> Image:\n",
    "        return transforms.Compose([\n",
    "            transforms.RandomCrop(size = 512),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomVerticalFlip()\n",
    "        ])(image)\n",
    "\n",
    "class ValidationDataSet(DataSetBase):\n",
    "    def __init__(self, high_resolution_image_path: Path, low_resolution_image_path: Path):\n",
    "        super().__init__(high_resolution_image_path)\n",
    "        self.high_resolution_image_path = high_resolution_image_path\n",
    "        self.low_resolution_image_path = low_resolution_image_path\n",
    "\n",
    "    def get_low_resolution_image(self, image: Image, path: Path)-> Image:\n",
    "        return PIL.Image.open(self.low_resolution_image_path / path.relative_to(self.high_resolution_image_path))\n",
    "\n",
    "def get_dataset() -> Tuple[TrainDataSet, ValidationDataSet]:\n",
    "    return TrainDataSet(Path(\"./dataset/train\"), 850 * 10), ValidationDataSet(Path(\"./dataset/validation/original\"), Path(\"./dataset/validation/0.25x\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データセットの画像確認(任意)\n",
    "データセットから数枚を表示して確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセットの確認(オプション/jupyter notebook専用コード)\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "from IPython.display import display\n",
    "to_image = transforms.ToPILImage()\n",
    "\n",
    "train_dataset, validation_dataset = get_dataset()\n",
    "images = [train_dataset[i] for i in range(5)]\n",
    "low_iamges = [i[0] for i in images]\n",
    "high_iamges = [i[1] for i in images]\n",
    "print(\"学習用画像/低解像度\")\n",
    "display(to_image(make_grid(low_iamges, nrow=5)))\n",
    "print(\"学習用画像/高解像度\")\n",
    "display(to_image(make_grid(high_iamges, nrow=5)))\n",
    "\n",
    "print(\"検証用画像/低解像度\")\n",
    "display(to_image(validation_dataset[0][0]))\n",
    "print(\"検証用画像/高解像度\")\n",
    "display(to_image(validation_dataset[0][1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step4 学習\n",
    "\n",
    "定義したモデルをpytorchで学習します。  \n",
    "バッチサイズなどのパラメーターはお使いのGPUのVRAMに合わせて調整をしてください。  \n",
    "\n",
    "学習時のログはlogフォルダーに保存されます。  \n",
    "\n",
    "学習後、ONNXモデルへ変換するためtorch.onnx.exportを呼び出しています。  \n",
    "この際、opset=17、モデルの入力名はinput、モデルの出力名はoutput、モデルの入力形状は(1, 3, height, width)となるように dynamic_axes を設定します。  \n",
    "(この例では(1, 3, 128, 128)のダミー入力を設定後、shape[2]、shape[3]にdynamic_axesを設定することで、モデルの入力形状を(1, 3, height, width)としています。)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習\n",
    "## パラメーター\n",
    "batch_size = 50\n",
    "num_workers = 0\n",
    "num_epoch = 100\n",
    "learning_rate = 1e-3\n",
    "## スクリプト本体\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torch.utils import data\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch.nn import MSELoss\n",
    "from torch import Tensor\n",
    "from tqdm.notebook import tqdm, trange \n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "to_image = transforms.ToPILImage()\n",
    "def calc_psnr(image1: Tensor, image2: Tensor):\n",
    "    image1 = cv2.cvtColor((np.array(to_image(image1))).astype(np.uint8), cv2.COLOR_RGB2BGR)\n",
    "    image2 = cv2.cvtColor((np.array(to_image(image2))).astype(np.uint8), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    return cv2.PSNR(image1, image2)\n",
    "    \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ESPCN4x()\n",
    "model.to(device)\n",
    "writer = SummaryWriter(\"log\")\n",
    "\n",
    "train_dataset, validation_dataset = get_dataset()\n",
    "train_data_loader = data.DataLoader(train_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True,\n",
    "                              num_workers=num_workers,\n",
    "                              pin_memory=True)\n",
    "validation_data_loader = data.DataLoader(validation_dataset,\n",
    "                              batch_size=1,\n",
    "                              shuffle=False,\n",
    "                              num_workers=num_workers)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = MultiStepLR(optimizer, milestones=[30, 50, 65, 80, 90], gamma=0.7) \n",
    "criterion = MSELoss()\n",
    "\n",
    "for epoch in trange(num_epoch, desc=\"EPOCH\"):\n",
    "    try:\n",
    "        # 学習\n",
    "        model.train()\n",
    "        train_loss = 0.0 \n",
    "        validation_loss = 0.0 \n",
    "        train_psnr = 0.0\n",
    "        validation_psnr = 0.0\n",
    "        for idx, (low_resolution_image, high_resolution_image ) in tqdm(enumerate(train_data_loader), desc=f\"EPOCH[{epoch}] TRAIN\", total=len(train_data_loader)):\n",
    "            low_resolution_image = low_resolution_image.to(device)\n",
    "            high_resolution_image = high_resolution_image.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(low_resolution_image)\n",
    "            loss = criterion(output, high_resolution_image)\n",
    "            loss.backward()\n",
    "            train_loss += loss.item() * low_resolution_image.size(0)\n",
    "            for image1, image2 in zip(output, high_resolution_image):   \n",
    "                train_psnr += calc_psnr(image1, image2)\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # 検証\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for idx, (low_resolution_image, high_resolution_image ) in tqdm(enumerate(validation_data_loader), desc=f\"EPOCH[{epoch}] VALIDATION\", total=len(validation_data_loader)):\n",
    "                low_resolution_image = low_resolution_image.to(device)\n",
    "                high_resolution_image = high_resolution_image.to(device)\n",
    "                output = model(low_resolution_image)\n",
    "                loss = criterion(output, high_resolution_image)\n",
    "                validation_loss += loss.item() * low_resolution_image.size(0)\n",
    "                for image1, image2 in zip(output, high_resolution_image):   \n",
    "                    validation_psnr += calc_psnr(image1, image2)\n",
    "            if epoch < 10 or epoch % 10 == 0:\n",
    "                display(to_image(low_resolution_image[0]))\n",
    "                display(to_image(high_resolution_image[0]))\n",
    "                display(to_image(output[0]))\n",
    "        writer.add_scalar(\"train/loss\", train_loss / len(train_dataset), epoch)\n",
    "        writer.add_scalar(\"train/psnr\", train_psnr / len(train_dataset), epoch)\n",
    "        writer.add_scalar(\"validation/loss\", validation_loss / len(validation_dataset), epoch)\n",
    "        writer.add_scalar(\"validation/psnr\", validation_psnr / len(validation_dataset), epoch)\n",
    "        writer.add_image(\"output\", output[0], epoch)\n",
    "    except Exception as ex:\n",
    "        print(f\"EPOCH[{epoch}] ERROR: {ex}\")\n",
    "\n",
    "writer.close()\n",
    "\n",
    "# モデル生成\n",
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "\n",
    "model.to(torch.device(\"cpu\"))\n",
    "dummy_input = torch.randn(1, 3, 128, 128, device=\"cpu\")\n",
    "torch.onnx.export(model, dummy_input, \"model.onnx\", \n",
    "                  opset_version=17,\n",
    "                  input_names=[\"input\"],\n",
    "                  output_names=[\"output\"],\n",
    "                  dynamic_axes={\"input\": {2: \"height\", 3:\"width\"}})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習結果のログ確認(任意)\n",
    "\n",
    "ログファイルに保存されている各epochの出力画像、loss、PSNRを確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard  --logdir log --host 0.0.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step5 ONNXモデルの動作確認\n",
    "\n",
    "pytorchで学習・変換したモデルをonnxruntimeで推論して確認します。  \n",
    "\n",
    "推論結果の画像はoutputフォルダーに生成されます。  \n",
    "また、簡易的ですが、手元環境での処理時間の計測も行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONNXモデルによる推論(SIGNATE上で動作させるものと同等)\n",
    "import onnxruntime as ort\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "input_image_dir = Path(\"dataset/validation/0.25x\")\n",
    "output_image_dir = Path(\"output\")\n",
    "output_image_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "sess = ort.InferenceSession(\"model.onnx\", providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\n",
    "input_images = []\n",
    "output_images = []\n",
    "output_paths = []\n",
    "\n",
    "print(\"load image\")\n",
    "for image_path in input_image_dir.iterdir():\n",
    "    output_iamge_path = output_image_dir / image_path.relative_to(input_image_dir)\n",
    "    input_image = cv2.imread(str(image_path))\n",
    "    input_image = np.array([cv2.cvtColor(input_image, cv2.COLOR_BGR2RGB).transpose((2,0,1))], dtype=np.float32)/255\n",
    "    input_images.append(input_image)\n",
    "    output_paths.append(output_iamge_path)\n",
    "\n",
    "print(\"inference\")\n",
    "start_time = datetime.datetime.now()\n",
    "for input_image in input_images:\n",
    "    output_images.append(sess.run([\"output\"], {\"input\": input_image})[0])\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "print(\"save image\")\n",
    "for output_path, output_image in zip(output_paths, output_images):\n",
    "    output_image = cv2.cvtColor((output_image.transpose((0,2,3,1))[0]*255).astype(np.uint8), cv2.COLOR_RGB2BGR)\n",
    "    cv2.imwrite(str(output_path), output_image)\n",
    "\n",
    "print(f\"inference time: {(end_time - start_time).total_seconds() / len(input_images)}[s/image]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step6 PSNRの計算\n",
    "\n",
    "onnxruntimeで推論した結果の画像に対してPSNRの計測を行います。  \n",
    "また、このスクリプトでは従来手法との比較も行います。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PSNR計算(従来手法との比較付き)\n",
    "import cv2\n",
    "from tqdm.notebook import tqdm\n",
    "original_image_dir = Path(\"dataset/validation/original\")\n",
    "output_label = [\"ESPCN\", \"NEAREST\", \"BILINEAR\", \"BICUBIC\"]\n",
    "output_psnr = [0.0, 0.0, 0.0, 0.0]\n",
    "original_image_paths = list(original_image_dir.iterdir())\n",
    "for image_path in tqdm(original_image_paths):\n",
    "    input_image_path = input_image_dir / image_path.relative_to(original_image_dir)\n",
    "    output_iamge_path = output_image_dir / image_path.relative_to(original_image_dir)\n",
    "    input_image = cv2.imread(str(input_image_path))\n",
    "    original_image = cv2.imread(str(image_path))\n",
    "    espcn_image = cv2.imread(str(output_iamge_path))\n",
    "    output_psnr[0] += cv2.PSNR(original_image, espcn_image)\n",
    "    h, w = original_image.shape[:2]\n",
    "    output_psnr[1] += cv2.PSNR(original_image, cv2.resize(input_image, (w, h), interpolation=cv2.INTER_NEAREST))\n",
    "    output_psnr[2] += cv2.PSNR(original_image, cv2.resize(input_image, (w, h), interpolation=cv2.INTER_LINEAR))\n",
    "    output_psnr[3] += cv2.PSNR(original_image, cv2.resize(input_image, (w, h), interpolation=cv2.INTER_CUBIC))\n",
    "\n",
    "# 拡大結果を表示\n",
    "from PIL import Image\n",
    "display(Image.fromarray(cv2.cvtColor(input_image, cv2.COLOR_BGR2RGB)))\n",
    "display(Image.fromarray(cv2.cvtColor(espcn_image, cv2.COLOR_BGR2RGB)))\n",
    "display(Image.fromarray(cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)))\n",
    "\n",
    "for label, psnr in zip(output_label, output_psnr):\n",
    "    print(f\"{label}: {psnr / len(original_image_paths)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
